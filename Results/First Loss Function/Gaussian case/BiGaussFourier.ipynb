{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "from Library.fourierABS import AnalyticalLossFunctionAbs\n",
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import quad, dblquad, IntegrationWarning\n",
    "import warnings\n",
    "import timeit\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use Fourier Transform to approximate different expectations in the constraints. More generally, if a function $f$ verifies certain conditions (see Analysis of Fourier transform valuation formulas and applications), we have the following:\n",
    "$$ \\mathbb{E}\\left[f(X - m)\\right] = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp((iu - R)m) \\mathbf{M}_X(R-iu)\\hat{f}(u+iR)$$\n",
    "where $\\mathbf{M}_X$ is the generating function of $X$. \n",
    "\n",
    "For $f(x) = (x^+)^2$, $\\hat{f}(u + iR) = \\frac{2}{i(u + iR)^3}$. For the cross terms, namely, $f(x,y) = x^+ y^+$, \n",
    "$\\hat{f}(u + iR) = \\frac{1}{u_1 + i R_1}\\frac{1}{u_2 + i R_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fourier_integral1d(cplx_integrand, j, x, eta):\n",
    "    def real_integrand(u):\n",
    "        return np.real(cplx_integrand(u, j, x, eta))\n",
    "\n",
    "    real_quad = quad(real_integrand, -np.inf, np.inf, epsrel=1e-4)[0]\n",
    "    return real_quad\n",
    "\n",
    "def fourier_integral2d(cplx_integrand, j, k, x, y, eta):\n",
    "    def real_integrand(u, v):\n",
    "        return np.real(cplx_integrand(u, v, j, k, x, y, eta))\n",
    "\n",
    "    real_quad = dblquad(real_integrand, \n",
    "                        -np.inf, np.inf,\n",
    "                        lambda x: -np.inf, lambda x: np.inf, \n",
    "                        epsrel=1e-4)[0]\n",
    "    return real_quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierGaussLossFunction(AnalyticalLossFunctionAbs):\n",
    "    def __init__(self, mu, sigma, alpha, c=None):\n",
    "        self.__mu = np.array(mu)\n",
    "        self.__sigma = np.array(sigma).reshape((len(mu), len(mu)))\n",
    "        self.__alpha = alpha\n",
    "        super(FourierGaussLossFunction, self).__init__(len(mu), c)\n",
    "\n",
    "    def moment_generating(self, t, i):\n",
    "        mu = self.__mu[i]\n",
    "        sigma2 = self.__sigma[i, i]\n",
    "        log_part = mu * t + 0.5 * sigma2 * t**2\n",
    "        return np.exp(log_part)\n",
    "\n",
    "    \n",
    "    def moment_generating2D(self, vec_t, i, j):\n",
    "        mu = self.__mu[[i, j]]\n",
    "        sigma2 = [[self.__sigma[i, i], self.__sigma[i, j]],\n",
    "                  [self.__sigma[j, i], self.__sigma[j, j]]]\n",
    "        log_part = np.dot(mu, vec_t)\n",
    "        log_part += 0.5 * np.dot(vec_t, np.dot(sigma2, vec_t))\n",
    "        return np.exp(log_part)\n",
    "    \n",
    "    def e(self, m):\n",
    "        return (self.__mu - m).sum()\n",
    "    \n",
    "    #this corresponds to the integrand when transforming the expectation corresponding to (x^+)**2\n",
    "    def g_fourier_integrand(self, u, j, m_j, eta):\n",
    "        i = 1j\n",
    "        eta_m_iu = eta - i * u\n",
    "        res = np.exp(-eta_m_iu * m_j)\n",
    "        res *= self.moment_generating(eta_m_iu, j)\n",
    "        res /= i * (u + i * eta) ** 3\n",
    "        return res\n",
    " \n",
    "\n",
    "\n",
    "    def g(self, i, m_i):\n",
    "        continue_bool = True\n",
    "        eta = 1.5 * np.random.rand()\n",
    "        while continue_bool:\n",
    "            try:\n",
    "                integral = fourier_integral1d(self.g_fourier_integrand, i, m_i, eta)\n",
    "                continue_bool = False\n",
    "                return 1. / np.pi * integral\n",
    "            except scipy.integrate.IntegrationWarning:\n",
    "                #print \"g not converging for x = %s, eta = %s\" % (m_i, eta)\n",
    "                eta = 1.5 * np.random.rand()\n",
    "\n",
    "    \n",
    "    #This corresponds to the integrand of the cross terms\n",
    "    def h_fourier_integrand(self, u, v, j, k, x, y, eta):\n",
    "        i = 1j\n",
    "        eta_m_iu = eta - i * np.array([u, v])\n",
    "        res = np.exp(np.dot(-eta_m_iu, [x, y]))\n",
    "        res *= self.moment_generating2D(eta_m_iu, j, k)\n",
    "        res /= (u + i*eta[0])**2 * (v + i*eta[1])**2\n",
    "        return res\n",
    "    \n",
    "\n",
    "    \n",
    "    def h(self, i, j, m_i, m_j):\n",
    "        continue_bool = True\n",
    "        eta = 1.5 * np.random.rand(2)\n",
    "        while continue_bool:\n",
    "            try:            \n",
    "                integral = fourier_integral2d(self.h_fourier_integrand, i, j, m_i, m_j, eta)\n",
    "                continue_bool = False\n",
    "                return (1 / (2 * np.pi)**2) * integral\n",
    "            except scipy.integrate.IntegrationWarning:\n",
    "                eta = 1.5 * np.random.rand(2)\n",
    "                \n",
    "\n",
    "    def jac_g_fourier_integrand(self, u, j, m_j, eta):\n",
    "        i = 1j\n",
    "        eta_m_iu = eta - i * u\n",
    "        res = np.exp(-eta_m_iu * m_j)\n",
    "        res *= self.moment_generating(eta_m_iu, j)\n",
    "        res /= (u + i * eta)**2\n",
    "        return res\n",
    "    \n",
    "    \n",
    "\n",
    "    def jac_g(self, i, m_i):\n",
    "        continue_bool = True\n",
    "        eta = 1.5 * np.random.rand()\n",
    "        while continue_bool:\n",
    "            try:\n",
    "                integral = fourier_integral1d(self.jac_g_fourier_integrand, i, m_i, eta)\n",
    "                continue_bool = False\n",
    "                return (-0.5 / np.pi) * integral\n",
    "            except scipy.integrate.IntegrationWarning:\n",
    "                #print \"f not converging for x = %s, alpha = %s\" % (m_i, eta)\n",
    "                eta = 1.5 * np.random.rand()\n",
    "                \n",
    "\n",
    "    def jac_h_fourier_integrand(self, u, v, j, k, x, y, eta):\n",
    "        i = 1j\n",
    "        eta_m_iu = eta - i * np.array([u, v])\n",
    "        res = np.exp(np.dot(-eta_m_iu, [x, y]))\n",
    "        res *= self.moment_generating2D(eta_m_iu, j, k)\n",
    "        res /= (u + i * eta[0]) ** 2 * (-eta_m_iu[1])\n",
    "        return res\n",
    "\n",
    "    def jac_h(self, i, j, m_i, m_j):\n",
    "        continue_bool = True\n",
    "        eta = 1.5 * np.random.rand(2)\n",
    "        while continue_bool:\n",
    "            try:            \n",
    "                integral = fourier_integral2d(self.jac_h_fourier_integrand, i, j, m_i, m_j, eta)\n",
    "                continue_bool = False\n",
    "                return (1 / (2 * np.pi)**2) * integral\n",
    "            except IntegrationWarning:\n",
    "                eta = 1.5 * np.random.rand(2)\n",
    "                \n",
    "                \n",
    "    def shortfall_risk(self, m=None):\n",
    "        m = self._check_argument(m)\n",
    "        sum_e = self.e(m)\n",
    "        sum_g, sum_h = 0., 0.\n",
    "        for i, m_i in enumerate(m):\n",
    "            sum_g += self.g(i, m_i)\n",
    "            if self.__alpha != 0:\n",
    "                for j, m_j in enumerate(m):\n",
    "                    if j > i:\n",
    "                        sum_h += self.h(i, j, m_i, m_j)  \n",
    "        return sum_e + 0.5 * sum_g + self.__alpha * sum_h\n",
    "    \n",
    "\n",
    "    \n",
    "    def shortfall_risk_jac(self, m):\n",
    "        m = self._check_argument(m)\n",
    "        res = []        \n",
    "        for i, m_i in enumerate(m):\n",
    "            partial_der = 1 + self.jac_g(i, m_i)\n",
    "            if self.__alpha != 0:\n",
    "                for j, m_j in enumerate(m):\n",
    "                    if i != j:\n",
    "                        partial_der += self.__alpha * self.jac_h(j, i, m_j, m_i)\n",
    "            res.append(partial_der)\n",
    "        return np.array(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.1 s ± 2.1 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "     fun: -0.2855887897197693\n",
      "     jac: array([1., 1.])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 5\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.1427944 , -0.14279439])\n"
     ]
    }
   ],
   "source": [
    "#Case: rho = -0.9, alpha = 1\n",
    "\n",
    "rho = -0.9\n",
    "\n",
    "mu = [0., 0.]\n",
    "sigma = [[1., rho], [rho, 1.]]\n",
    "\n",
    "c = 1.\n",
    "alpha  = 1.\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "maxiter = 3500\n",
    "\n",
    "init = np.ones(loss.dim)\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "cons = ({'type': 'ineq',\n",
    "         'fun' : lambda x: loss.ineq_constraint(x),\n",
    "         'jac' : lambda x: loss.ineq_constraint_jac(x)})\n",
    "\n",
    "%timeit res = minimize(loss.objective, init, jac=loss.objective_jac, constraints=cons, method='SLSQP',options={'maxiter': maxiter})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3 s ± 2.33 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "     fun: -0.2855887897197693\n",
      "     jac: array([1., 1.])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 5\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.1427944 , -0.14279439])\n"
     ]
    }
   ],
   "source": [
    "#Case: rho = -0.5, alpha = 1\n",
    "\n",
    "rho = -0.5\n",
    "\n",
    "mu = [0., 0.]\n",
    "sigma = [[1., rho], [rho, 1.]]\n",
    "\n",
    "c = 1.\n",
    "alpha  = 1.\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "maxiter = 3500\n",
    "\n",
    "init = np.ones(loss.dim)\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "cons = ({'type': 'ineq',\n",
    "         'fun' : lambda x: loss.ineq_constraint(x),\n",
    "         'jac' : lambda x: loss.ineq_constraint_jac(x)})\n",
    "\n",
    "\n",
    "\n",
    "#r to specify how many times to repeat and n number of times the function is called\n",
    "%timeit res = minimize(loss.objective, init, jac=loss.objective_jac, constraints=cons, method='SLSQP',options={'maxiter': maxiter})\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.59 s ± 881 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "     fun: -0.2855887897197693\n",
      "     jac: array([1., 1.])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 5\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.1427944 , -0.14279439])\n"
     ]
    }
   ],
   "source": [
    "#Case: rho = -0.2, alpha = 1\n",
    "\n",
    "rho = -0.2\n",
    "\n",
    "mu = [0., 0.]\n",
    "sigma = [[1., rho], [rho, 1.]]\n",
    "\n",
    "c = 1.\n",
    "alpha  = 1.\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "maxiter = 3500\n",
    "\n",
    "init = np.ones(loss.dim)\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "cons = ({'type': 'ineq',\n",
    "         'fun' : lambda x: loss.ineq_constraint(x),\n",
    "         'jac' : lambda x: loss.ineq_constraint_jac(x)})\n",
    "\n",
    "\n",
    "\n",
    "#r to specify how many times to repeat and n number of times the function is called\n",
    "%timeit res = minimize(loss.objective, init, jac=loss.objective_jac, constraints=cons, method='SLSQP',options={'maxiter': maxiter})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.79 s ± 1.65 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "     fun: -0.2855887897197693\n",
      "     jac: array([1., 1.])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 5\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.1427944 , -0.14279439])\n"
     ]
    }
   ],
   "source": [
    "#Case: rho = 0, alpha = 1\n",
    "\n",
    "rho = 0\n",
    "\n",
    "mu = [0., 0.]\n",
    "sigma = [[1., rho], [rho, 1.]]\n",
    "\n",
    "c = 1.\n",
    "alpha  = 1.\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "maxiter = 3500\n",
    "\n",
    "init = np.ones(loss.dim)\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "cons = ({'type': 'ineq',\n",
    "         'fun' : lambda x: loss.ineq_constraint(x),\n",
    "         'jac' : lambda x: loss.ineq_constraint_jac(x)})\n",
    "\n",
    "\n",
    "\n",
    "#r to specify how many times to repeat and n number of times the function is called\n",
    "%timeit res = minimize(loss.objective, init, jac=loss.objective_jac, constraints=cons, method='SLSQP',options={'maxiter': maxiter})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.74 s ± 1.05 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "     fun: -0.2855887897197693\n",
      "     jac: array([1., 1.])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 5\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.1427944 , -0.14279439])\n"
     ]
    }
   ],
   "source": [
    "#Case: rho = 0.2, alpha = 1\n",
    "\n",
    "rho = 0.2\n",
    "\n",
    "mu = [0., 0.]\n",
    "sigma = [[1., rho], [rho, 1.]]\n",
    "\n",
    "c = 1.\n",
    "alpha  = 1.\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "maxiter = 3500\n",
    "\n",
    "init = np.ones(loss.dim)\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "cons = ({'type': 'ineq',\n",
    "         'fun' : lambda x: loss.ineq_constraint(x),\n",
    "         'jac' : lambda x: loss.ineq_constraint_jac(x)})\n",
    "\n",
    "\n",
    "\n",
    "#r to specify how many times to repeat and n number of times the function is called\n",
    "%timeit res = minimize(loss.objective, init, jac=loss.objective_jac, constraints=cons, method='SLSQP',options={'maxiter': maxiter})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2 s ± 1.48 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "     fun: -0.2855887897197693\n",
      "     jac: array([1., 1.])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 5\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.1427944 , -0.14279439])\n"
     ]
    }
   ],
   "source": [
    "#Case: rho = 0.5, alpha = 1\n",
    "\n",
    "rho = 0.5\n",
    "\n",
    "mu = [0., 0.]\n",
    "sigma = [[1., rho], [rho, 1.]]\n",
    "\n",
    "c = 1.\n",
    "alpha  = 1.\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "maxiter = 3500\n",
    "\n",
    "init = np.ones(loss.dim)\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "cons = ({'type': 'ineq',\n",
    "         'fun' : lambda x: loss.ineq_constraint(x),\n",
    "         'jac' : lambda x: loss.ineq_constraint_jac(x)})\n",
    "\n",
    "\n",
    "\n",
    "#r to specify how many times to repeat and n number of times the function is called\n",
    "%timeit res = minimize(loss.objective, init, jac=loss.objective_jac, constraints=cons, method='SLSQP',options={'maxiter': maxiter})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8 s ± 1.09 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "     fun: -0.2855887897197693\n",
      "     jac: array([1., 1.])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 5\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.1427944 , -0.14279439])\n"
     ]
    }
   ],
   "source": [
    "#Case: rho = 0.9, alpha = 1\n",
    "\n",
    "rho = 0.9\n",
    "\n",
    "mu = [0., 0.]\n",
    "sigma = [[1., rho], [rho, 1.]]\n",
    "\n",
    "c = 1.\n",
    "alpha  = 1.\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "maxiter = 3500\n",
    "\n",
    "init = np.ones(loss.dim)\n",
    "loss = FourierGaussLossFunction(mu, sigma, alpha, c)\n",
    "cons = ({'type': 'ineq',\n",
    "         'fun' : lambda x: loss.ineq_constraint(x),\n",
    "         'jac' : lambda x: loss.ineq_constraint_jac(x)})\n",
    "\n",
    "\n",
    "\n",
    "#r to specify how many times to repeat and n number of times the function is called\n",
    "%timeit res = minimize(loss.objective, init, jac=loss.objective_jac, constraints=cons, method='SLSQP',options={'maxiter': maxiter})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
